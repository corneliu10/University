{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'detect_peaks.py', 'evaluators.py', 'kernel1b66f5b77c.ipynb', 'load_data.py', 'logistic_regression.py', 'ml-unibuc-2019-23.zip', 'naive_bayes.py', 'perceptron.py', 'sample_submission.csv', 'signal_analysis_utils.py', 'siml', 'sk_utils.py', 'test', 'train', 'train_labels.csv', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import math\n",
    "print(os.listdir(\"./\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "train_root_path = './train'\n",
    "train_labels_path = './train_labels.csv'\n",
    "test_root_path = './test'\n",
    "\n",
    "N = 135\n",
    "f_s = 100\n",
    "t_n = 1.35\n",
    "T = t_n / N\n",
    "\n",
    "train_samples = 9000\n",
    "test_samples = 5000\n",
    "n_features = 3\n",
    "bucket_size = 135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 3)\n",
      "(100, 135, 3)\n",
      "(100,)\n",
      "[10001. 10002. 10004. ... 23992. 23998. 24000.]\n"
     ]
    }
   ],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None)\n",
    "    return np.resize(dataframe.values, (bucket_size, n_features))\n",
    "\n",
    "# test load_file\n",
    "print(load_file(os.path.join(train_root_path, '16763.csv')).shape)\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(root_path, limit = None):\n",
    "    loaded = list()\n",
    "    filenames = os.listdir(root_path)\n",
    "    if limit is None: \n",
    "        limit = len(filenames)\n",
    "    for i, name in enumerate(filenames):\n",
    "        if i >= limit: break\n",
    "        data = load_file(os.path.join(root_path, name))\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = np.stack(loaded, axis=0)\n",
    "    return loaded\n",
    "\n",
    "# test load_group\n",
    "print(load_group(train_root_path, limit=100).shape)\n",
    "\n",
    "# load train labels\n",
    "def load_train_labels(labels_path, train_data_root_path, limit = train_samples):\n",
    "    train_labels = np.zeros(limit)\n",
    "    loaded = pd.read_csv(labels_path)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(train_data_root_path)):\n",
    "        if i >= limit: break\n",
    "        file_id = int(filename.split('.')[0], 10)\n",
    "        train_labels[i] = int(loaded[loaded['id'] == file_id]['class'])\n",
    "        \n",
    "    return train_labels.reshape(limit, )\n",
    "\n",
    "# test load_train_labels\n",
    "print(load_train_labels(train_labels_path, train_root_path, limit = 100).shape)\n",
    "\n",
    "# load test ids\n",
    "def load_test_ids(test_root_path):\n",
    "    test_ids = np.zeros(test_samples)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(test_root_path)):\n",
    "        file_id = int(filename.split('.')[0], 10)\n",
    "        test_ids[i] = file_id\n",
    "        \n",
    "    return test_ids\n",
    "\n",
    "# test load_test_ids\n",
    "print(load_test_ids(test_root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 135, 3)\n",
      "(9000,)\n",
      "(5000, 135, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# load data from input\n",
    "train_data = load_group(train_root_path)\n",
    "train_labels = load_train_labels(train_labels_path, train_root_path) - 1\n",
    "test_data = load_group(test_root_path)\n",
    "test_ids = load_test_ids(test_root_path)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_data\n",
    "# train_data, test_data, train_labels, test_labels = train_test_split(train_data, train_labels, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siml.sk_utils import *\n",
    "from siml.signal_analysis_utils import *\n",
    "\n",
    "# denominator = 10\n",
    "# X_train, Y_train = extract_features_labels(train_data, train_labels, T, N, f_s, denominator)\n",
    "# X_test, Y_test = extract_features_labels(test_data, test_labels, T, N, f_s, denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 675)\n",
      "(5000, 675)\n"
     ]
    }
   ],
   "source": [
    "def extract_features(data):\n",
    "    feature_engin = [np.mean(data, axis=2), np.sum(data**2, axis=2)]\n",
    "\n",
    "    new_data = []\n",
    "    for feature in feature_engin:\n",
    "        new_data = np.append(new_data, feature)\n",
    "            \n",
    "    return new_data.reshape(len(data), len(feature_engin) * bucket_size)\n",
    "\n",
    "# train_features = extract_features(train_data)\n",
    "# test_features = extract_features(test_data)\n",
    "train_features = np.reshape(train_data, (train_data.shape[0], train_data.shape[1]*train_data.shape[2]))\n",
    "test_features = np.reshape(test_data, (test_data.shape[0], test_data.shape[1]*test_data.shape[2]))\n",
    "train_features = np.append(train_features, extract_features(train_data), axis = 1)\n",
    "test_features = np.append(test_features, extract_features(test_data), axis = 1)\n",
    "\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gravitation_orientation():\n",
    "    global train_data, test_data\n",
    "    # add gravitational orientation as a feature to train_data\n",
    "    train_data = np.insert(train_data, 3, 0, axis=2)\n",
    "    for i in train_data:\n",
    "        for j in i:\n",
    "            j[3] = math.sqrt(j[0]**2 + j[1]**2 + j[2]**2)\n",
    "\n",
    "    print(train_data.shape)\n",
    "\n",
    "    # add gravitational orientation as a feature to test_data\n",
    "    test_data = np.insert(test_data, 3, 0, axis=2)\n",
    "    for i in test_data:\n",
    "        for j in i:\n",
    "            j[3] = math.sqrt(j[0]**2 + j[1]**2 + j[2]**2)\n",
    "\n",
    "    print(test_data.shape)\n",
    "\n",
    "# add_gravitation_orientation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators=1000)\n",
    "# clf.fit(X_train, Y_train)\n",
    "# print(\"Accuracy on training set is : {}\".format(clf.score(X_train, Y_train)))\n",
    "# print(\"Accuracy on test set is : {}\".format(clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Gradient Boosting Classifier in 72.40 s\n",
      "trained Random Forest in 0.16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corneliu.dumitru\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\corneliu.dumitru\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\corneliu.dumitru\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Logistic Regression in 5.63 s\n",
      "trained Nearest Neighbors in 0.03 s\n",
      "trained Decision Tree in 0.63 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.554</td>\n",
       "      <td>72.397855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.155805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.025310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.628595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.325</td>\n",
       "      <td>5.631273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  train_score  test_score  train_time\n",
       "0  Gradient Boosting Classifier        1.000       0.554   72.397855\n",
       "1                 Random Forest        1.000       0.533    0.155805\n",
       "3             Nearest Neighbors        0.657       0.495    0.025310\n",
       "4                 Decision Tree        1.000       0.382    0.628595\n",
       "2           Logistic Regression        1.000       0.325    5.631273"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_results = batch_classify(train_features[:1000], train_labels[:1000], test_features[:1000], test_labels[:1000])\n",
    "display_dict_models(dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(train_data, test_data, type=None):\n",
    "    if type == None:\n",
    "        return train_data, test_data\n",
    "    \n",
    "    if type == 'standard':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(train_data)\n",
    "    \n",
    "    if type == 'minmax':\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)) # (0, 1) default\n",
    "        scaler.fit(train_data)\n",
    "        \n",
    "    if type == 'l1':\n",
    "        train_data_l1 = preprocessing.normalize(train_data, norm='l1', axis=1)\n",
    "        test_data_l1 = preprocessing.normalize(test_data, norm='l1', axis=1)\n",
    "        \n",
    "        return train_data_l1, test_data_l1\n",
    "    \n",
    "    if type == 'l2':\n",
    "        train_data_l2 = preprocessing.normalize(train_data, norm='l2', axis=1)\n",
    "        test_data_l2 = preprocessing.normalize(test_data, norm='l2', axis=1)\n",
    "        \n",
    "        return train_data_l2, test_data_l2\n",
    "    \n",
    "    train_data_scaled = scaler.transform(train_data)\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "        \n",
    "    return train_data_scaled, test_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(train_data, train_labels, test_data, C):\n",
    "    modelSVM = svm.SVC(C, \"linear\")\n",
    "    modelSVM.fit(train_data, train_labels)\n",
    "    \n",
    "    train_labels_predicted = modelSVM.predict(train_data)\n",
    "    test_labels_predicted = modelSVM.predict(test_data)\n",
    "    \n",
    "    return train_labels_predicted, test_labels_predicted\n",
    "\n",
    "def compute_accuracy(true_labels, predicted_labels):\n",
    "    return (true_labels == predicted_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy (standard,0.1): 1.0\n",
      "Test accuracy (standard,0.1): 0.593\n",
      "Train accuracy (standard,0.5): 1.0\n",
      "Test accuracy (standard,0.5): 0.593\n",
      "Train accuracy (standard,1): 1.0\n",
      "Test accuracy (standard,1): 0.593\n",
      "Train accuracy (standard,10): 1.0\n",
      "Test accuracy (standard,10): 0.593\n",
      "Train accuracy (l2,0.1): 0.371\n",
      "Test accuracy (l2,0.1): 0.275\n",
      "Train accuracy (l2,0.5): 0.667\n",
      "Test accuracy (l2,0.5): 0.481\n",
      "Train accuracy (l2,1): 0.773\n",
      "Test accuracy (l2,1): 0.512\n",
      "Train accuracy (l2,10): 0.988\n",
      "Test accuracy (l2,10): 0.543\n",
      "Train accuracy (minmax,0.1): 0.825\n",
      "Test accuracy (minmax,0.1): 0.517\n",
      "Train accuracy (minmax,0.5): 0.979\n",
      "Test accuracy (minmax,0.5): 0.548\n",
      "Train accuracy (minmax,1): 0.996\n",
      "Test accuracy (minmax,1): 0.546\n",
      "Train accuracy (minmax,10): 1.0\n",
      "Test accuracy (minmax,10): 0.543\n",
      "[[[1.    1.    1.    1.   ]\n",
      "  [0.593 0.593 0.593 0.593]]\n",
      "\n",
      " [[0.371 0.667 0.773 0.988]\n",
      "  [0.275 0.481 0.512 0.543]]\n",
      "\n",
      " [[0.825 0.979 0.996 1.   ]\n",
      "  [0.517 0.548 0.546 0.543]]]\n"
     ]
    }
   ],
   "source": [
    "def evaluateSVM(train_data, train_labels, test_data, test_labels):\n",
    "    C = [1e-1, 0.5, 1, 10]\n",
    "    norm_types = ['standard', 'l2', 'minmax']\n",
    "    accuracy = np.zeros((len(norm_types), 2, len(C)))\n",
    "\n",
    "    for j in range(len(norm_types)):\n",
    "        for i in range(len(C)):\n",
    "            train_data, test_data = normalize_data(train_data, test_data, type=norm_types[j])\n",
    "            train_labels_predicted, test_labels_predicted = svm_classifier(train_data, train_labels, test_data, C[i])\n",
    "            \n",
    "            print(\"Train accuracy (\" + norm_types[j] + \",\" + str(C[i]) +\"): \" + str(compute_accuracy(train_labels, train_labels_predicted)))\n",
    "            print(\"Test accuracy (\" + norm_types[j] + \",\" + str(C[i]) +\"): \" + str(compute_accuracy(test_labels, test_labels_predicted)))\n",
    "            \n",
    "            accuracy[j, 0, i] = compute_accuracy(train_labels, train_labels_predicted)\n",
    "            accuracy[j, 1, i] = compute_accuracy(test_labels, test_labels_predicted)\n",
    "    \n",
    "    print(accuracy)\n",
    "    \n",
    "evaluateSVM(train_features[:1000], train_labels[:1000], test_features[:1000], test_labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corneliu.dumitru\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-ad2368c50626>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtest_labels_predicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtest_labels_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_RandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-ad2368c50626>\u001b[0m in \u001b[0;36mevaluate_RandomForest\u001b[1;34m(train_data, train_labels, test_data, test_labels)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtest_labels_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels_predicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels_predicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtest_labels_predicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-7255108a0020>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[1;34m(true_labels, predicted_labels)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrue_labels\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "def evaluate_RandomForest(train_data, train_labels, test_data, test_labels):\n",
    "    rf = RandomForestClassifier(n_estimators = 1000)\n",
    "    rf.fit(train_data, train_labels)\n",
    "    \n",
    "    train_labels_predicted = rf.predict(train_data)\n",
    "    test_labels_predicted = rf.predict(test_data)\n",
    "    print(\"Train accuracy: \", compute_accuracy(train_labels, train_labels_predicted))\n",
    "    print(\"Test accuracy: \", compute_accuracy(test_labels, test_labels_predicted))\n",
    "    \n",
    "    return test_labels_predicted\n",
    "\n",
    "test_labels_predicted = evaluate_RandomForest(train_features, train_labels, test_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "def evaluate_model_LSTM(trainX, trainY):\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainY.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(trainX, trainY, batch_size=batch_size, verbose=0)\n",
    "    print(accuracy)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model_ConvLSTM2D(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 150, 128\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    \n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length = 4, 37\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    print(accuracy)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate random forrest model\n",
    "def evaluate_model_RandomForrest(trainX, trainY, testX, testY):\n",
    "    # Create a Gaussian Classifier\n",
    "    model = RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "    # Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Accuracy on training set is : {}\".format(model.score(trainX, trainY)))\n",
    "    print(\"Accuracy on test set is : {}\".format(model.score(testX, testY)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "# model = evaluate_model_RandomForrest(X_train, Y_train.ravel(), X_test, Y_test.ravel())\n",
    "# model = evaluate_model_ConvLSTM2D(train_data, train_labels, test_data, test_labels)\n",
    "model = evaluate_model_ConvLSTM2D(train_data, train_labels, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with different configs\n",
    "4 features \n",
    "verbose, epochs, batch_size = 0, 25, 64      -> 0.847\n",
    "\n",
    "\n",
    "3 features \n",
    "verbose, epochs, batch_size = 0, 25, 64      -> 0.8795\n",
    "\n",
    "\n",
    "3 features \n",
    "verbose, epochs, batch_size = 0, 35, 128     -> 0.8845\n",
    "\n",
    "\n",
    "3 features\n",
    "verbose, epochs, batch_size = 1, 50, 128     -> 0.8984\n",
    "\n",
    "\n",
    "3 features\n",
    "verbose, epochs, batch_size = 1, 100, 128    -> 0.9115\n",
    "\n",
    "3 features + std\n",
    "verbose, epochs, batch_size = 0, 100, 128    -> 0.9044\n",
    "\n",
    "3 features + std\n",
    "verbose, epochs, batch_size = 0, 150, 128    -> 0.9194\n",
    "\n",
    "3 features + std\n",
    "bucket size = 145\n",
    "verbose, epochs, batch_size = 0, 150, 128    -> 0.9439\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test_data\n",
    "def predictConvLSTM(model, testX):\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length, n_features = 4, 37, testX.shape[2]\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    predictions = model.predict(testX)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_including_test_data():\n",
    "    global model, train_data, test_data, train_labels\n",
    "    \n",
    "    test_labels = predictConvLSTM(model, test_data)\n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    \n",
    "    print(train_labels.shape)\n",
    "    print(test_labels.shape)\n",
    "\n",
    "    train_data = np.concatenate((train_data, test_data))\n",
    "    train_labels = np.concatenate((train_labels, test_labels))\n",
    "\n",
    "    model = evaluate_model_ConvLSTM2D(train_data, train_labels, train_data, train_labels)    \n",
    "    \n",
    "evaluate_including_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predict\n",
    "predictions = predictConvLSTM(model, test_data)\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_labels_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-7db3e93651f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels_predicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_labels_predicted' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_labels_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id' : [], 'class' : []}\n",
    "for i in range(len(predictions)):\n",
    "    d['id'].append(int(test_ids[i]))\n",
    "    d['class'].append(np.argmax(predictions[i]) + 1)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test_labels csv file\n",
    "\n",
    "dataframe = pd.DataFrame(data=d)\n",
    "dataframe.to_csv('test_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
