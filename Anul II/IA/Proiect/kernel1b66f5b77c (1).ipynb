{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'detect_peaks.py', 'evaluators.py', 'kernel1b66f5b77c (1).ipynb', 'kernel1b66f5b77c.ipynb', 'load_data.py', 'logistic_regression.py', 'ml-unibuc-2019-23.zip', 'naive_bayes.py', 'perceptron.py', 'sample_submission.csv', 'signal_analysis_utils.py', 'siml', 'sk_utils.py', 'test', 'test_labels.csv', 'train', 'train_labels.csv', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import math\n",
    "print(os.listdir(\".\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "train_root_path = './train'\n",
    "train_labels_path = './train_labels.csv'\n",
    "test_root_path = './test'\n",
    "\n",
    "train_samples = 9000\n",
    "test_samples = 5000\n",
    "n_features = 3\n",
    "bucket_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "(100, 150, 3)\n",
      "(100,)\n",
      "[10001. 10002. 10004. ... 23992. 23998. 24000.]\n"
     ]
    }
   ],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None)\n",
    "    return np.resize(dataframe.values, (bucket_size, n_features))\n",
    "\n",
    "# test load_file\n",
    "print(load_file(os.path.join(train_root_path, '16763.csv')).shape)\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(root_path, limit = None):\n",
    "    loaded = list()\n",
    "    filenames = os.listdir(root_path)\n",
    "    if limit is None: \n",
    "        limit = len(filenames)\n",
    "    for i, name in enumerate(filenames):\n",
    "        if i >= limit: break\n",
    "        data = load_file(os.path.join(root_path, name))\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = np.stack(loaded, axis=0)\n",
    "    return loaded\n",
    "\n",
    "# test load_group\n",
    "print(load_group(train_root_path, limit=100).shape)\n",
    "\n",
    "# load train labels\n",
    "def load_train_labels(labels_path, train_data_root_path, limit = train_samples):\n",
    "    train_labels = np.zeros(limit)\n",
    "    loaded = pd.read_csv(labels_path)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(train_data_root_path)):\n",
    "        if i >= limit: break\n",
    "        file_id = int(filename.split('.')[0], 10)\n",
    "        train_labels[i] = int(loaded[loaded['id'] == file_id]['class'])\n",
    "        \n",
    "    return train_labels.reshape(limit, )\n",
    "\n",
    "# test load_train_labels\n",
    "print(load_train_labels(train_labels_path, train_root_path, limit = 100).shape)\n",
    "\n",
    "# load test ids\n",
    "def load_test_ids(test_root_path):\n",
    "    test_ids = np.zeros(test_samples)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(test_root_path)):\n",
    "        file_id = int(filename.split('.')[0], 10)\n",
    "        test_ids[i] = file_id\n",
    "        \n",
    "    return test_ids\n",
    "\n",
    "# test load_test_ids\n",
    "print(load_test_ids(test_root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 150, 3)\n",
      "(9000,)\n",
      "(5000, 150, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# load data from input\n",
    "train_data = load_group(train_root_path)\n",
    "train_labels = load_train_labels(train_labels_path, train_root_path) - 1\n",
    "test_data = load_group(test_root_path)\n",
    "test_ids = load_test_ids(test_root_path)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_data\n",
    "# train_data, test_data, train_labels, test_labels = train_test_split(train_data, train_labels, test_size=0.20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_data, train_labels, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 150, 4)\n",
      "(5000, 150, 4)\n"
     ]
    }
   ],
   "source": [
    "# X_test = np.append(X_test, np.reshape(np.mean(X_test, axis=2), (X_test.shape[0], X_test.shape[1], 1)), axis=2)\n",
    "# X_train = np.append(X_train, np.reshape(np.mean(X_train, axis=2), (X_train.shape[0], X_train.shape[1], 1)), axis=2)\n",
    "\n",
    "test_data = np.append(test_data, np.reshape(np.mean(test_data, axis=2), (test_data.shape[0], test_data.shape[1], 1)), axis=2)\n",
    "train_data = np.append(train_data, np.reshape(np.mean(train_data, axis=2), (train_data.shape[0], train_data.shape[1], 1)), axis=2)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# print(X_test.shape)\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 20)\n"
     ]
    }
   ],
   "source": [
    "def encode_labels(labels):\n",
    "    labels = to_categorical(labels)\n",
    "    print(labels.shape)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# one hot encode train_labels y\n",
    "train_labels = encode_labels(train_labels)\n",
    "# Y_train = encode_labels(Y_train)\n",
    "\n",
    "# one hot encode test_labels y\n",
    "# Y_test = encode_labels(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "def standardize_data(data):\n",
    "    data -= np.mean(data)\n",
    "    data /= np.std(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# X_train = standardize_data(X_train)\n",
    "# X_test = standardize_data(X_test)\n",
    "train_data = standardize_data(train_data)\n",
    "test_data = standardize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model_ConvLSTM2D(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 1, 100, 128\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    \n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length = 5, 30\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='tanh', input_shape=(n_steps, 1, n_length, n_features), ))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    print(accuracy)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 30s 3ms/step - loss: 1.9374 - acc: 0.3524\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 1.3383 - acc: 0.5312\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 1.0550 - acc: 0.6202\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.9541 - acc: 0.6507\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.8042 - acc: 0.7034\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.7544 - acc: 0.7191\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.7051 - acc: 0.7408\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 28s 3ms/step - loss: 0.6459 - acc: 0.7640\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 40s 4ms/step - loss: 0.6034 - acc: 0.7807\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 41s 5ms/step - loss: 0.5861 - acc: 0.7829\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 0.5700 - acc: 0.7912\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 0.6016 - acc: 0.7771\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 36s 4ms/step - loss: 0.5098 - acc: 0.8137\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 28s 3ms/step - loss: 0.5618 - acc: 0.7923\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.5001 - acc: 0.8144\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.4823 - acc: 0.8240\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 30s 3ms/step - loss: 0.4358 - acc: 0.8437\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4256 - acc: 0.8437\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4171 - acc: 0.8486\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 22s 2ms/step - loss: 0.4362 - acc: 0.8393\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.3950 - acc: 0.8562\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.3882 - acc: 0.8619\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.3608 - acc: 0.8681\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.3395 - acc: 0.8787\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.3363 - acc: 0.8773\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.3321 - acc: 0.8804\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.3323 - acc: 0.8794\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.2956 - acc: 0.8962\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.2890 - acc: 0.8966\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.2711 - acc: 0.9038\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 49s 5ms/step - loss: 0.2773 - acc: 0.9017\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 31s 3ms/step - loss: 0.2634 - acc: 0.9084\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.2515 - acc: 0.9111\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.2608 - acc: 0.9070\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.2233 - acc: 0.9232\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.2118 - acc: 0.9269\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.2113 - acc: 0.9281\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.2130 - acc: 0.9266\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.2047 - acc: 0.9290\n",
      "Epoch 40/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.1941 - acc: 0.9342\n",
      "Epoch 41/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.2047 - acc: 0.9302\n",
      "Epoch 42/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1772 - acc: 0.9417\n",
      "Epoch 43/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1908 - acc: 0.9331\n",
      "Epoch 44/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1774 - acc: 0.9396\n",
      "Epoch 45/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1749 - acc: 0.9410\n",
      "Epoch 46/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1626 - acc: 0.9441\n",
      "Epoch 47/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1660 - acc: 0.9444\n",
      "Epoch 48/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1714 - acc: 0.9383\n",
      "Epoch 49/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1583 - acc: 0.9467\n",
      "Epoch 50/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1576 - acc: 0.9461\n",
      "Epoch 51/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1482 - acc: 0.9517\n",
      "Epoch 52/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1271 - acc: 0.9604\n",
      "Epoch 53/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1201 - acc: 0.9609\n",
      "Epoch 54/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1325 - acc: 0.9557\n",
      "Epoch 55/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1328 - acc: 0.9553\n",
      "Epoch 56/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1270 - acc: 0.9596\n",
      "Epoch 57/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1069 - acc: 0.9679\n",
      "Epoch 58/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1098 - acc: 0.9653\n",
      "Epoch 59/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1132 - acc: 0.9623\n",
      "Epoch 60/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.0969 - acc: 0.9689\n",
      "Epoch 61/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1112 - acc: 0.9628\n",
      "Epoch 62/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1200 - acc: 0.9603\n",
      "Epoch 63/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1010 - acc: 0.9682\n",
      "Epoch 64/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.1016 - acc: 0.9662\n",
      "Epoch 65/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1016 - acc: 0.9666\n",
      "Epoch 66/100\n",
      "9000/9000 [==============================] - 27s 3ms/step - loss: 0.1132 - acc: 0.9608\n",
      "Epoch 67/100\n",
      "9000/9000 [==============================] - 27s 3ms/step - loss: 0.0871 - acc: 0.9727\n",
      "Epoch 68/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.0816 - acc: 0.9761\n",
      "Epoch 69/100\n",
      "9000/9000 [==============================] - 27s 3ms/step - loss: 0.0865 - acc: 0.9727\n",
      "Epoch 70/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.1105 - acc: 0.9623\n",
      "Epoch 71/100\n",
      "9000/9000 [==============================] - 27s 3ms/step - loss: 0.0811 - acc: 0.9753\n",
      "Epoch 72/100\n",
      "9000/9000 [==============================] - 28s 3ms/step - loss: 0.0723 - acc: 0.9776\n",
      "Epoch 73/100\n",
      "9000/9000 [==============================] - 28s 3ms/step - loss: 0.0796 - acc: 0.9742\n",
      "Epoch 74/100\n",
      "9000/9000 [==============================] - 29s 3ms/step - loss: 0.0798 - acc: 0.9728\n",
      "Epoch 75/100\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 0.0741 - acc: 0.9766\n",
      "Epoch 76/100\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 0.0823 - acc: 0.9737\n",
      "Epoch 77/100\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.0719 - acc: 0.9764\n",
      "Epoch 78/100\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.0813 - acc: 0.9739\n",
      "Epoch 79/100\n",
      "9000/9000 [==============================] - 48s 5ms/step - loss: 0.0657 - acc: 0.9799\n",
      "Epoch 80/100\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 0.0602 - acc: 0.9828\n",
      "Epoch 81/100\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 0.0565 - acc: 0.9837\n",
      "Epoch 82/100\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.0636 - acc: 0.9804\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 34s 4ms/step - loss: 0.0595 - acc: 0.9830\n",
      "Epoch 84/100\n",
      "8704/9000 [============================>.] - ETA: 1s - loss: 0.0716 - acc: 0.9777"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-8f78dbace766>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# evaluate a model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# model = evaluate_model_ConvLSTM2D(X_train[:1000], Y_train[:1000], X_test[:1000], Y_test[:1000])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model_ConvLSTM2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-186-046b70482f2d>\u001b[0m in \u001b[0;36mevaluate_model_ConvLSTM2D\u001b[1;34m(trainX, trainy, testX, testy)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluate a model\n",
    "# model = evaluate_model_ConvLSTM2D(X_train[:1000], Y_train[:1000], X_test[:1000], Y_test[:1000])\n",
    "model = evaluate_model_ConvLSTM2D(train_data, train_labels, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test_data\n",
    "def predictConvLSTM(model, testX):\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length, n_features = 5, 30, testX.shape[2]\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    predictions = model.predict(testX)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predict\n",
    "predictions = predictConvLSTM(model, test_data)\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id' : [], 'class' : []}\n",
    "for i in range(len(predictions)):\n",
    "    d['id'].append(int(test_ids[i]))\n",
    "    d['class'].append(np.argmax(predictions[i]) + 1)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test_labels csv file\n",
    "\n",
    "dataframe = pd.DataFrame(data=d)\n",
    "dataframe.to_csv('test_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
