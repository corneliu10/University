{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'kernel1b66f5b77c.ipynb', 'ml-unibuc-2019-23.zip', 'sample_submission.csv', 'test', 'train', 'train_labels.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import math\n",
    "print(os.listdir(\"./\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "train_root_path = './train'\n",
    "train_labels_path = './train_labels.csv'\n",
    "test_root_path = './test'\n",
    "\n",
    "N = 135\n",
    "f_s = 100\n",
    "t_n = 1.35\n",
    "T = t_n / N\n",
    "\n",
    "train_samples = 9000\n",
    "test_samples = 5000\n",
    "n_features = 3\n",
    "bucket_size = 145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 3)\n",
      "(145, 3)\n"
     ]
    }
   ],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None)\n",
    "    return np.resize(dataframe.values, (bucket_size, n_features))\n",
    "\n",
    "# test load_file\n",
    "print(load_file(os.path.join(train_root_path, '16763.csv')).shape)\n",
    "print(load_file(os.path.join(test_root_path, '17928.csv')).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 145, 3)\n"
     ]
    }
   ],
   "source": [
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(root_path, limit = None):\n",
    "    loaded = list()\n",
    "    filenames = os.listdir(root_path)\n",
    "    if limit is None: \n",
    "        limit = len(filenames)\n",
    "    for i, name in enumerate(filenames):\n",
    "        if i >= limit: break\n",
    "        data = load_file(os.path.join(root_path, name))\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = np.stack(loaded, axis=0)\n",
    "    return loaded\n",
    "\n",
    "# test load_group\n",
    "print(load_group(train_root_path, limit=100).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "# load train labels\n",
    "def load_train_labels(labels_path, train_data_root_path, limit = train_samples):\n",
    "    train_labels = np.zeros(limit)\n",
    "    loaded = pd.read_csv(labels_path)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(train_data_root_path)):\n",
    "        if i >= limit: break\n",
    "        file_id = int(filename.split('.')[0], 10)\n",
    "        train_labels[i] = int(loaded[loaded['id'] == file_id]['class'])\n",
    "        \n",
    "    return train_labels.reshape(limit, 1)\n",
    "\n",
    "# test load_train_labels\n",
    "print(load_train_labels(train_labels_path, train_root_path, limit = 100).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10001. 10002. 10004. ... 23992. 23998. 24000.]\n"
     ]
    }
   ],
   "source": [
    "# load test ids\n",
    "def load_test_ids(test_root_path):\n",
    "    test_ids = np.zeros(test_samples)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(test_root_path)):\n",
    "        file_id = int(filename.split('.')[0], 10)\n",
    "        test_ids[i] = file_id\n",
    "        \n",
    "    return test_ids\n",
    "\n",
    "# test load_test_ids\n",
    "print(load_test_ids(test_root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 145, 3)\n",
      "(100, 1)\n",
      "(5000, 145, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# load data from input\n",
    "train_data = load_group(train_root_path, limit=100)\n",
    "train_labels = load_train_labels(train_labels_path, train_root_path, limit=100) - 1\n",
    "test_data = load_group(test_root_path)\n",
    "test_ids = load_test_ids(test_root_path)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "def standardize_data(data):\n",
    "    data -= np.mean(data)\n",
    "    data /= np.std(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = standardize_data(train_data)\n",
    "test_data = standardize_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---- SPLIT TRAIN_DATA ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train_data into 90% - 10% test_data\n",
    "def split_train_data():\n",
    "    global train_data, test_data\n",
    "    global train_labels, test_labels\n",
    "    \n",
    "    train_data, test_data = np.array_split(train_data, [train_samples - 1000])\n",
    "    train_labels, test_labels = np.array_split(train_labels, [train_samples - 1000])\n",
    "\n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(test_labels.shape)\n",
    "    \n",
    "# split_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gravitation_orientation():\n",
    "    global train_data, test_data\n",
    "    # add gravitational orientation as a feature to train_data\n",
    "    train_data = np.insert(train_data, 3, 0, axis=2)\n",
    "    for i in train_data:\n",
    "        for j in i:\n",
    "            j[3] = math.sqrt(j[0]**2 + j[1]**2 + j[2]**2)\n",
    "\n",
    "    print(train_data.shape)\n",
    "\n",
    "    # add gravitational orientation as a feature to test_data\n",
    "    test_data = np.insert(test_data, 3, 0, axis=2)\n",
    "    for i in test_data:\n",
    "        for j in i:\n",
    "            j[3] = math.sqrt(j[0]**2 + j[1]**2 + j[2]**2)\n",
    "\n",
    "    print(test_data.shape)\n",
    "\n",
    "# add_gravitation_orientation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n"
     ]
    }
   ],
   "source": [
    "def encode_labels(labels):\n",
    "    labels = to_categorical(labels)\n",
    "    print(labels.shape)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# one hot encode train_labels y\n",
    "train_labels = encode_labels(train_labels)\n",
    "\n",
    "# one hot encode test_labels y\n",
    "# test_labels = encode_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "def evaluate_model_LSTM(trainX, trainY):\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainY.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(trainX, trainY, batch_size=batch_size, verbose=0)\n",
    "    print(accuracy)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model_ConvLSTM2D(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 150, 128\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    \n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length = 5, 29\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    print(accuracy)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate random forrest model\n",
    "def evaluate_model_RandomForrest(trainX, trainY, testX, testY):\n",
    "    # Create a Gaussian Classifier\n",
    "    model = RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "    # Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Accuracy on training set is : {}\".format(model.score(trainX, trainY)))\n",
    "    print(\"Accuracy on test set is : {}\".format(model.score(testX, testY)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744\n"
     ]
    }
   ],
   "source": [
    "# evaluate a model\n",
    "# model = evaluate_model_RandomForrest(X_train, Y_train.ravel(), X_test, Y_test.ravel())\n",
    "# model = evaluate_model_ConvLSTM2D(train_data, train_labels, test_data, test_labels)\n",
    "model = evaluate_model_ConvLSTM2D(train_data, train_labels, test_data, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with different configs\n",
    "4 features \n",
    "verbose, epochs, batch_size = 0, 25, 64      -> 0.847\n",
    "\n",
    "\n",
    "3 features \n",
    "verbose, epochs, batch_size = 0, 25, 64      -> 0.8795\n",
    "\n",
    "\n",
    "3 features \n",
    "verbose, epochs, batch_size = 0, 35, 128     -> 0.8845\n",
    "\n",
    "\n",
    "3 features\n",
    "verbose, epochs, batch_size = 1, 50, 128     -> 0.8984\n",
    "\n",
    "\n",
    "3 features\n",
    "verbose, epochs, batch_size = 1, 100, 128    -> 0.9115\n",
    "\n",
    "3 features + std\n",
    "verbose, epochs, batch_size = 0, 100, 128    -> 0.9044\n",
    "\n",
    "3 features + std\n",
    "verbose, epochs, batch_size = 0, 150, 128    -> 0.9194\n",
    "\n",
    "3 features + std\n",
    "bucket size = 145\n",
    "verbose, epochs, batch_size = 0, 150, 128    -> 0.9439\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 145, 3)\n",
      "(5000, 145, 3)\n",
      "(100, 20)\n",
      "(5000, 20)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_including_test_data():\n",
    "    global model, train_data, test_data, train_labels\n",
    "    \n",
    "    test_labels = predictConvLSTM(model, test_data)\n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    \n",
    "    print(train_labels.shape)\n",
    "    print(test_labels.shape)\n",
    "\n",
    "    train_data = np.concatenate((train_data, test_data))\n",
    "    train_labels = np.concatenate((train_labels, test_labels))\n",
    "\n",
    "    model = evaluate_model_ConvLSTM2D(train_data, train_labels, train_data, train_labels)    \n",
    "    \n",
    "evaluate_including_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions for test_data\n",
    "def predictConvLSTM(model, testX):\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length, n_features = 5, 29, testX.shape[2]\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    predictions = model.predict(testX)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# test predict\n",
    "predictions = predictConvLSTM(model, test_data)\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id' : [], 'class' : []}\n",
    "for i in range(len(predictions)):\n",
    "    d['id'].append(int(test_ids[i]))\n",
    "    d['class'].append(np.argmax(predictions[i]) + 1)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test_labels csv file\n",
    "\n",
    "dataframe = pd.DataFrame(data=d)\n",
    "dataframe.to_csv('test_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
