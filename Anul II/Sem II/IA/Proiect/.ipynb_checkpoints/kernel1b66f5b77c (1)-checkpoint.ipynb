{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'detect_peaks.py', 'evaluators.py', 'kernel1b66f5b77c (1).ipynb', 'kernel1b66f5b77c.ipynb', 'load_data.py', 'logistic_regression.py', 'ml-unibuc-2019-23.zip', 'naive_bayes.py', 'perceptron.py', 'sample_submission.csv', 'signal_analysis_utils.py', 'siml', 'sk_utils.py', 'test', 'test_labels.csv', 'train', 'train_labels.csv', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import math\n",
    "print(os.listdir(\".\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "train_root_path = './train'\n",
    "train_labels_path = './train_labels.csv'\n",
    "test_root_path = './test'\n",
    "\n",
    "train_samples = 9000\n",
    "test_samples = 5000\n",
    "n_features = 3\n",
    "bucket_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "(100, 150, 3)\n",
      "(100,)\n",
      "[10001. 10002. 10004. ... 23992. 23998. 24000.]\n"
     ]
    }
   ],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None)\n",
    "    return np.resize(dataframe.values, (bucket_size, n_features))\n",
    "\n",
    "# test load_file\n",
    "print(load_file(os.path.join(train_root_path, '16763.csv')).shape)\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(root_path, limit = None):\n",
    "    loaded = list()\n",
    "    filenames = os.listdir(root_path)\n",
    "    if limit is None: \n",
    "        limit = len(filenames)\n",
    "    for i, name in enumerate(filenames):\n",
    "        if i >= limit: break\n",
    "        data = load_file(os.path.join(root_path, name))\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = np.stack(loaded, axis=0)\n",
    "    return loaded\n",
    "\n",
    "# test load_group\n",
    "print(load_group(train_root_path, limit=100).shape)\n",
    "\n",
    "# load train labels\n",
    "def load_train_labels(labels_path, train_data_root_path, limit = train_samples):\n",
    "    train_labels = np.zeros(limit)\n",
    "    loaded = pd.read_csv(labels_path)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(train_data_root_path)):\n",
    "        if i >= limit: break\n",
    "        file_id = int(filename.split('.')[0], 10)\n",
    "        train_labels[i] = int(loaded[loaded['id'] == file_id]['class'])\n",
    "        \n",
    "    return train_labels.reshape(limit, )\n",
    "\n",
    "# test load_train_labels\n",
    "print(load_train_labels(train_labels_path, train_root_path, limit = 100).shape)\n",
    "\n",
    "# load test ids\n",
    "def load_test_ids(test_root_path):\n",
    "    test_ids = np.zeros(test_samples)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(test_root_path)):\n",
    "        file_id = int(filename.split('.')[0], 10)\n",
    "        test_ids[i] = file_id\n",
    "        \n",
    "    return test_ids\n",
    "\n",
    "# test load_test_ids\n",
    "print(load_test_ids(test_root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 150, 3)\n",
      "(9000,)\n",
      "(5000, 150, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# load data from input\n",
    "train_data = load_group(train_root_path)\n",
    "train_labels = load_train_labels(train_labels_path, train_root_path) - 1\n",
    "test_data = load_group(test_root_path)\n",
    "test_ids = load_test_ids(test_root_path)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_data\n",
    "# train_data, test_data, train_labels, test_labels = train_test_split(train_data, train_labels, test_size=0.20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_data, train_labels, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 150, 4)\n",
      "(5000, 150, 4)\n"
     ]
    }
   ],
   "source": [
    "# X_test = np.append(X_test, np.reshape(np.mean(X_test, axis=2), (X_test.shape[0], X_test.shape[1], 1)), axis=2)\n",
    "# X_train = np.append(X_train, np.reshape(np.mean(X_train, axis=2), (X_train.shape[0], X_train.shape[1], 1)), axis=2)\n",
    "\n",
    "test_data = np.append(test_data, np.reshape(np.mean(test_data, axis=2), (test_data.shape[0], test_data.shape[1], 1)), axis=2)\n",
    "train_data = np.append(train_data, np.reshape(np.mean(train_data, axis=2), (train_data.shape[0], train_data.shape[1], 1)), axis=2)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# print(X_test.shape)\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 20)\n"
     ]
    }
   ],
   "source": [
    "def encode_labels(labels):\n",
    "    labels = to_categorical(labels)\n",
    "    print(labels.shape)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# one hot encode train_labels y\n",
    "train_labels = encode_labels(train_labels)\n",
    "# Y_train = encode_labels(Y_train)\n",
    "\n",
    "# one hot encode test_labels y\n",
    "# Y_test = encode_labels(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "def standardize_data(data):\n",
    "    data -= np.mean(data)\n",
    "    data /= np.std(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# X_train = standardize_data(X_train)\n",
    "# X_test = standardize_data(X_test)\n",
    "train_data = standardize_data(train_data)\n",
    "test_data = standardize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model_ConvLSTM2D(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 1, 100, 128\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    \n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length = 5, 30\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='tanh', input_shape=(n_steps, 1, n_length, n_features), ))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    print(accuracy)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 30s 3ms/step - loss: 1.9374 - acc: 0.3524\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 1.3383 - acc: 0.5312\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 1.0550 - acc: 0.6202\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.9541 - acc: 0.6507\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.8042 - acc: 0.7034\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.7544 - acc: 0.7191\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.7051 - acc: 0.7408\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 28s 3ms/step - loss: 0.6459 - acc: 0.7640\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 40s 4ms/step - loss: 0.6034 - acc: 0.7807\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 41s 5ms/step - loss: 0.5861 - acc: 0.7829\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 0.5700 - acc: 0.7912\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 0.6016 - acc: 0.7771\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 36s 4ms/step - loss: 0.5098 - acc: 0.8137\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 28s 3ms/step - loss: 0.5618 - acc: 0.7923\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.5001 - acc: 0.8144\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 25s 3ms/step - loss: 0.4823 - acc: 0.8240\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 30s 3ms/step - loss: 0.4358 - acc: 0.8437\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4256 - acc: 0.8437\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 21s 2ms/step - loss: 0.4171 - acc: 0.8486\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 22s 2ms/step - loss: 0.4362 - acc: 0.8393\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.3950 - acc: 0.8562\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.3882 - acc: 0.8619\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.3608 - acc: 0.8681\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.3395 - acc: 0.8787\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 23s 3ms/step - loss: 0.3363 - acc: 0.8773\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.3321 - acc: 0.8804\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.3323 - acc: 0.8794\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.2956 - acc: 0.8962\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.2890 - acc: 0.8966\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 24s 3ms/step - loss: 0.2711 - acc: 0.9038\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 49s 5ms/step - loss: 0.2773 - acc: 0.9017\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 31s 3ms/step - loss: 0.2634 - acc: 0.9084\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.2515 - acc: 0.9111\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 26s 3ms/step - loss: 0.2608 - acc: 0.9070\n",
      "Epoch 35/100\n",
      "1536/9000 [====>.........................] - ETA: 20s - loss: 0.2134 - acc: 0.9245"
     ]
    }
   ],
   "source": [
    "# evaluate a model\n",
    "# model = evaluate_model_ConvLSTM2D(X_train[:1000], Y_train[:1000], X_test[:1000], Y_test[:1000])\n",
    "model = evaluate_model_ConvLSTM2D(train_data, train_labels, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test_data\n",
    "def predictConvLSTM(model, testX):\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length, n_features = 5, 30, testX.shape[2]\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    predictions = model.predict(testX)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predict\n",
    "predictions = predictConvLSTM(model, test_data)\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id' : [], 'class' : []}\n",
    "for i in range(len(predictions)):\n",
    "    d['id'].append(int(test_ids[i]))\n",
    "    d['class'].append(np.argmax(predictions[i]) + 1)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test_labels csv file\n",
    "\n",
    "dataframe = pd.DataFrame(data=d)\n",
    "dataframe.to_csv('test_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
